# erd_EEG  
**Project ID**  
ERD

**Experimental location**   
Montreal Neurological Institute, Montreal, QC

**Task Description**   
Developmental research using electroencephalography (EEG) offers valuable insights in brain processes early in life, but at the same time, applying this sensitive technique to young children who are often non-compliant and have short attention spans comes with pratical limitations. It is thus of particular importance to optimally use the limited resources to advance our understanding of development through reproducible and replicable research practices. Here, we describe methodological approaches that help maximize the reproducibility of developmental EEG research. We discuss how to transform EEG data into the standardized Brain Imaging Data Structure (BIDS) which organizes data according to the FAIR data sharing principles. We provide a tutorial on how to use cluster-based permutation testing to analyze developmental EEG data. This versatile test statistic solves the multiple comparison problem omnipresent in EEG analysis and thereby substantially decreases the risk of reporting false discoveries. Finally, we describe how to quantify effect sizes, in particular of cluster-based permutation results. Reporting effect sizes conveys a finding’s impact and robustness which in turn informs future research. To demonstrate these methodological approaches to data organization, analysis and report, we use a publicly accessible infant EEG dataset and provide a complete copy of the analysis code. This collection contains the results of the analysis and a static copy of the analysis scripts. See also https://doi.org/10.34973/gvr3-6g88 for the raw data.

**Participant categories**   
This study involves a total of 59 full-term infants, specifically aged 9 months (M = 272.78 days, range: 251–289 days). Participants were recruited from local families and were screened to ensure there were no indications of atypical development. Out of the initial group, 48 infants were included in the final analysis after excluding participants based on strict criteria: those who had more than 70% of their trials rejected due to artifacts or poor data quality. The inclusion criteria ensured that only infants without developmental concerns participated, while the exclusion criteria helped maintain high data quality, addressing challenges such as short attention spans, excessive movement during recording, and higher dropout rates typical in this age group. 

**Research Paper**  
https://www.sciencedirect.com/science/article/pii/S1878929321001250

**Contact Person**  
Robert Oostenveld    
r.oostenveld@donders.ru.nl
